{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37431921",
   "metadata": {},
   "source": [
    "# 3. VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8341610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fbab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e972e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝 데이터셋을 다운로드한다.\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5)),\n",
    "])\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "# 테스트 데이터셋을 다운로드한다.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8687ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.Dropout(0.5))\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7fe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):\n",
    "    # Initialize weights for CNNs\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, arch):\n",
    "        super(VGG, self).__init__()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks, nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.Dropout(0.5), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.Dropout(0.5), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(10))\n",
    "    \n",
    "    def para_init(self):\n",
    "        self.net.apply(init_cnn)\n",
    "        \n",
    "    # 포워드 패스    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f74dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (net): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Dropout(p=0.5, inplace=False)\n",
      "      (2): LazyConv2d(0, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "    (6): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.5, inplace=False)\n",
      "    (10): LazyLinear(in_features=0, out_features=4096, bias=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): ReLU()\n",
      "    (13): Dropout(p=0.5, inplace=False)\n",
      "    (14): LazyLinear(in_features=0, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSSA_16\\anaconda3\\envs\\smplify_x4\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:175: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "print(VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4ac4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimzer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # 모델을 훈련 모드로 설정\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X) # 포워드 패스 수행\n",
    "        loss = loss_fn(pred, y) # CE 연산\n",
    "        \n",
    "        optimzer.zero_grad() # 0 으로 초기화\n",
    "        loss.backward() # 역전파하여 그래디언트 계산\n",
    "        optimzer.step() # 연산된 그래디언트를 사용해 파라미터를 업데이트\n",
    "        \n",
    "        if batch % 100 == 0: # 매 100회차 마다 다음 내용 출력\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            #print(f'loss: {loss}, [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval() # 모델을 실행 모드로 설정\n",
    "    \n",
    "    with torch.no_grad(): # 그래디언트 연산 안함\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = model(X) # 포워드 패스 수행\n",
    "            test_loss += loss_fn(pred, y) # CE 연산\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # 결과 일치하는지 확인\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}% Average Loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e456c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(device):\n",
    "    #device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   \n",
    "    #device = 'cpu'\n",
    "    print(f\"사용할 장치: {device}\")\n",
    "\n",
    "    model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128))).to(device)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f'Epoch {t+1}\\n--------------------------------------')\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        test_loop(train_dataloader, model, loss_fn, device)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e7a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용할 장치: cuda:0\n",
      "Epoch 1\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 10.2% Average Loss: 2.302033\n",
      "\n",
      "Epoch 2\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 28.6% Average Loss: 2.294591\n",
      "\n",
      "Epoch 3\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.4% Average Loss: 1.804507\n",
      "\n",
      "Epoch 4\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.6% Average Loss: 1.530882\n",
      "\n",
      "Epoch 5\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.9% Average Loss: 1.445886\n",
      "\n",
      "Epoch 6\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 65.4% Average Loss: 1.315120\n",
      "\n",
      "Epoch 7\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.9% Average Loss: 1.278793\n",
      "\n",
      "Epoch 8\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.1% Average Loss: 1.192643\n",
      "\n",
      "Epoch 9\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.4% Average Loss: 1.137711\n",
      "\n",
      "Epoch 10\n",
      "--------------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.7% Average Loss: 1.103930\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "run(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3fac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smplify_x4",
   "language": "python",
   "name": "smplify_x4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
